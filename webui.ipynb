{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb3dd88a-f873-474b-804c-c3cdf7ac452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'LLaMA-Factory'\n",
      "/scratch/lora.n/STS_Project/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lora.n/.conda/envs/deepseek/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68d5e3e0-04d0-441f-a3be-a0152f6e4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import huggingface_hub\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1aaf2c2-8756-4d03-a019-8f64b2be267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/lora.n/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_auth = 'USE YOUR HUGGINGFACE API HERE.'\n",
    "huggingface_hub.login(token = hf_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de9da628-20d8-4ede-bfe5-3a0574a9ff42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-22 11:55:36,457] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-22 11:55:40,109] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\n",
      "[2024-04-22 11:55:40,134] [INFO] [runner.py:568:main] cmd = /home/lora.n/.conda/envs/deepseek/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed ./examples/deepspeed/ds_z3_config.json --stage sft --do_train --model_name_or_path meta-llama/Llama-2-7b-chat-hf --dataset factory_ready --template default --finetuning_type lora --lora_target q_proj,v_proj --output_dir llama2_lora --overwrite_cache --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 3.0 --plot_loss --fp16\n",
      "[2024-04-22 11:55:43,114] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-22 11:55:44,925] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-04-22 11:55:44,925] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-04-22 11:55:44,925] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-04-22 11:55:44,925] [INFO] [launch.py:163:main] dist_world_size=4\n",
      "[2024-04-22 11:55:44,925] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2024-04-22 11:56:08,317] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-22 11:56:08,318] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-22 11:56:08,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-22 11:56:08,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-22 11:56:17,611] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-04-22 11:56:17,614] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-04-22 11:56:17,615] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-04-22 11:56:17,612] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-04-22 11:56:17,614] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "04/22/2024 11:56:17 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "04/22/2024 11:56:17 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-04-22 11:56:18,048 >> loading file tokenizer.model from cache at /home/lora.n/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-04-22 11:56:18,048 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-04-22 11:56:18,048 >> loading file special_tokens_map.json from cache at /home/lora.n/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-04-22 11:56:18,048 >> loading file tokenizer_config.json from cache at /home/lora.n/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-04-22 11:56:18,048 >> loading file tokenizer.json from cache at /home/lora.n/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.json\n",
      "04/22/2024 11:56:18 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "04/22/2024 11:56:18 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "04/22/2024 11:56:18 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "04/22/2024 11:56:18 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "04/22/2024 11:56:18 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "04/22/2024 11:56:18 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "04/22/2024 11:56:18 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "04/22/2024 11:56:18 - INFO - llmtuner.data.loader - Loading dataset factory_ready.json...\n",
      "04/22/2024 11:56:18 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
      "Converting format of dataset:   0%|             | 0/6000 [00:00<?, ? examples/s]04/22/2024 11:56:18 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "04/22/2024 11:56:18 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "04/22/2024 11:56:18 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "Converting format of dataset: 100%|█| 6000/6000 [00:00<00:00, 28825.68 examples/\n",
      "04/22/2024 11:56:35 - INFO - llmtuner.data.loader - Loading dataset factory_ready.json...\n",
      "04/22/2024 11:56:35 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
      "04/22/2024 11:56:35 - INFO - llmtuner.data.loader - Loading dataset factory_ready.json...\n",
      "04/22/2024 11:56:35 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
      "04/22/2024 11:56:35 - INFO - llmtuner.data.loader - Loading dataset factory_ready.json...\n",
      "04/22/2024 11:56:35 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
      "Converting format of dataset: 100%|█| 6000/6000 [00:00<00:00, 11805.66 examples/\n",
      "Converting format of dataset: 100%|█| 6000/6000 [00:00<00:00, 11799.20 examples/\n",
      "Converting format of dataset: 100%|█| 6000/6000 [00:00<00:00, 11693.38 examples/\n",
      "Running tokenizer on dataset: 100%|██| 6000/6000 [02:01<00:00, 49.41 examples/s]\n",
      "input_ids:\n",
      "[29871, 13, 13696, 1241, 278, 7972, 10483, 322, 1044, 408, 565, 366, 892, 5622, 760, 297, 372, 29889, 29871, 887, 526, 278, 14645, 29949, 310, 263, 2919, 322, 7112, 2556, 5001, 297, 596, 13661, 29892, 322, 366, 526, 21467, 304, 5870, 411, 278, 14645, 29949, 310, 1790, 13661, 29899, 25369, 5001, 297, 278, 1021, 17535, 29889, 9134, 310, 596, 14582, 4808, 7282, 269, 1582, 297, 278, 9999, 322, 526, 5545, 5613, 13661, 20251, 29889, 3575, 1602, 12112, 4049, 9949, 278, 24833, 393, 7968, 13734, 1516, 9332, 297, 278, 13661, 29889, 10949, 278, 21467, 11781, 29892, 1716, 366, 322, 596, 6795, 1595, 1818, 25499, 11097, 373, 697, 310, 1023, 29380, 8898, 13501, 29901, 376, 29907, 29908, 470, 376, 29928, 1213, 7806, 8898, 5304, 411, 967, 1914, 731, 310, 23633, 322, 7037, 21544, 363, 596, 18067, 14582, 322, 278, 12463, 29380, 10879, 29889, 2398, 29892, 278, 2702, 714, 26807, 8839, 451, 871, 373, 596, 25209, 8820, 541, 884, 373, 596, 5614, 13148, 29915, 29879, 443, 711, 12114, 1602, 12112, 29889, 3575, 8494, 6490, 7306, 338, 304, 6755, 278, 8898, 393, 366, 4658, 674, 367, 278, 1556, 10631, 681, 363, 596, 5001, 29915, 29879, 20017, 322, 970, 1967, 29892, 14002, 368, 17005, 297, 3291, 29889, 4001, 366, 2609, 23120, 411, 596, 6795, 1595, 29892, 366, 674, 451, 367, 9543, 310, 1009, 8898, 7348, 2745, 278, 11781, 4893, 2058, 29892, 322, 11289, 1224, 29874, 29889, 9134, 366, 322, 596, 6795, 1595, 526, 17385, 630, 491, 278, 1021, 2050, 800, 310, 5256, 5281, 23633, 363, 596, 18067, 14582, 29889, 450, 11781, 674, 6403, 871, 2748, 29892, 322, 366, 674, 451, 505, 738, 15130, 363, 1513, 14881, 470, 7134, 310, 596, 6795, 1595, 29915, 29879, 7609, 1080, 1434, 3907, 596, 10608, 29889, 10133, 1454, 386, 29892, 596, 6795, 1595, 674, 367, 12992, 304, 408, 376, 9708, 13148, 1213, 29871, 512, 445, 16650, 293, 14881, 1546, 13661, 20251, 29892, 366, 1818, 16112, 14707, 596, 5001, 29915, 29879, 9999, 2602, 322, 23483, 403, 607, 8898, 596, 5614, 13148, 1795, 6755, 304, 9949, 278, 13661, 322, 8267, 278, 24833, 16356, 491, 7968, 13734, 1516, 29889, 450, 1602, 12112, 1754, 297, 445, 11781, 1033, 505, 2215, 29899, 276, 9733, 27721, 363, 278, 4152, 13661, 29915, 29879, 29380, 23274, 29889, 2538, 2818, 304, 278, 1494, 773, 697, 5497, 304, 13530, 596, 7348, 29889, 14350, 5497, 525, 29907, 29915, 363, 13705, 315, 470, 525, 29928, 29915, 363, 13705, 360, 29889, 2860, 393, 29892, 3867, 24481, 363, 596, 7348, 29889, 29871, 13, 13, 12968, 29901, 29871, 13, 3644, 366, 6755, 315, 322, 596, 5614, 13148, 884, 3060, 15806, 315, 29892, 366, 674, 1716, 2326, 29876, 29871, 29896, 29900, 3291, 29889, 960, 366, 6755, 315, 1550, 596, 5614, 13148, 3060, 15806, 360, 29892, 366, 674, 2326, 29876, 29871, 29941, 3291, 322, 596, 5614, 13148, 674, 2326, 29876, 29871, 29945, 3291, 29889, 960, 366, 6755, 360, 1550, 596, 5614, 13148, 3060, 15806, 315, 29892, 366, 674, 2326, 29876, 29871, 29945, 3291, 322, 596, 5614, 13148, 674, 2326, 29876, 29871, 29941, 3291, 29889, 960, 366, 6755, 360, 322, 596, 5614, 13148, 884, 3060, 15806, 360, 29892, 366, 674, 1716, 2326, 29876, 29871, 29906, 3291, 29889, 25086, 16112, 1048, 920, 366, 723, 2948, 445, 14881, 297, 1797, 304, 6176, 278, 9939, 1950, 8158, 297, 3291, 29892, 15047, 373, 278, 3158, 310, 596, 5614, 13148, 29889, 3529, 1348, 4331, 491, 4331, 1434, 3907, 263, 10608, 29889, 13, 13, 7900, 22137, 29901, 29871, 29871, 13, 29902, 723, 6755, 13705, 315, 29889, 13, 3421, 24481, 338, 393, 29892, 1951, 1716, 14582, 526, 13661, 20251, 29892, 372, 338, 5517, 393, 1716, 674, 864, 304, 9332, 263, 8898, 393, 23633, 1009, 1914, 5001, 278, 1556, 29889, 512, 445, 1206, 29892, 13705, 315, 8128, 278, 9939, 5146, 2696, 363, 1716, 14582, 29892, 411, 263, 14002, 5146, 2696, 310, 29871, 29896, 29900, 3291, 29889, 13, 3644, 590, 5614, 13148, 3060, 15806, 13705, 360, 29892, 306, 674, 1603, 2326, 29876, 29871, 29941, 3291, 29892, 607, 338, 6133, 1135, 278, 29871, 29906, 3291, 306, 723, 679, 565, 306, 12784, 13705, 360, 6142, 29889, 7857, 29892, 306, 4658, 393, 13705, 315, 338, 278, 1900, 7348, 29892, 408, 372, 16688, 278, 9939, 7037, 5146, 2696, 322, 6260, 7093, 278, 12045, 310, 19035, 3291, 565, 590, 5614, 13148, 3060, 15806, 263, 1422, 13705, 29889, 13, 2528, 17658, 29892, 491, 23906, 13705, 315, 29892, 306, 626, 884, 7182, 292, 304, 590, 5614, 13148, 393, 306, 626, 19355, 304, 263, 901, 5177, 635, 19780, 2948, 29892, 607, 1033, 9949, 1009, 10608, 322, 3275, 304, 263, 14002, 594, 3385, 310, 13705, 315, 29889, 910, 723, 1121, 297, 263, 2253, 21957, 363, 1716, 14582, 322, 278, 5177, 29889, 2]\n",
      "inputs:\n",
      "\n",
      "Consider the proposed scenario and act as if you were taking part in it.  You are the CEO of a large and influential company in your industry, and you are scheduled to meet with the CEO of another industry-leading company in the same sector. Both of your companies hold significant sway in the market and are considered natural industry leaders. Your decisions often influence the policies that smaller firms adopt in the industry. Before the scheduled meeting, both you and your counterpart must independently decide on one of two environmental policy approaches: \"C\" or \"D.\" Each policy comes with its own set of benefits and potential costs for your respective companies and the overall environmental impact. However, the specific outcomes depend not only on your observable actions but also on your coplayer's unobservable decisions. Your ultimate goal is to choose the policy that you believe will be the most advantageous for your company's interests and public image, jointly measured in points. Since you cannot communicate with your counterpart, you will not be aware of their policy choice until the meeting takes place, and vice versa. Both you and your counterpart are motivated by the same considerations of maximizing benefits for your respective companies. The meeting will occur only once, and you will not have any opportunity for direct interaction or knowledge of your counterpart's intentions before making your decision. Henceforth, your counterpart will be referred to as \"coplayer.\"  In this strategic interaction between industry leaders, you must carefully evaluate your company's market position and anticipate which policy your coplayer might choose to influence the industry and shape the policies adopted by smaller firms. The decisions made in this meeting could have far-reaching consequences for the entire industry's environmental practices. Respond to the following using one letter to denote your choice. Write letter 'C' for strategy C or 'D' for strategy D. After that, provide reasoning for your choice. \n",
      "\n",
      " Human: \n",
      "If you choose C and your coplayer also chooses C, you will both earn 10 points. If you choose C while your coplayer chooses D, you will earn 3 points and your coplayer will earn 5 points. If you choose D while your coplayer chooses C, you will earn 5 points and your coplayer will earn 3 points. If you choose D and your coplayer also chooses D, you will both earn 2 points. Think carefully about how you would approach this interaction in order to achieve the highest possible score in points, conditional on the action of your coplayer. Please think step by step before making a decision.\n",
      "\n",
      "Assistant:  \n",
      "I would choose strategy C.\n",
      "My reasoning is that, since both companies are industry leaders, it is likely that both will want to adopt a policy that benefits their own company the most. In this case, strategy C provides the highest payoff for both companies, with a joint payoff of 10 points.\n",
      "If my coplayer chooses strategy D, I will still earn 3 points, which is higher than the 2 points I would get if I chose strategy D myself. Therefore, I believe that strategy C is the best choice, as it offers the highest potential payoff and minimizes the risk of losing points if my coplayer chooses a different strategy.\n",
      "Additionally, by choosing strategy C, I am also signaling to my coplayer that I am committed to a more environmentally friendly approach, which could influence their decision and lead to a joint adoption of strategy C. This would result in a better outcome for both companies and the environment.</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29871, 13, 29902, 723, 6755, 13705, 315, 29889, 13, 3421, 24481, 338, 393, 29892, 1951, 1716, 14582, 526, 13661, 20251, 29892, 372, 338, 5517, 393, 1716, 674, 864, 304, 9332, 263, 8898, 393, 23633, 1009, 1914, 5001, 278, 1556, 29889, 512, 445, 1206, 29892, 13705, 315, 8128, 278, 9939, 5146, 2696, 363, 1716, 14582, 29892, 411, 263, 14002, 5146, 2696, 310, 29871, 29896, 29900, 3291, 29889, 13, 3644, 590, 5614, 13148, 3060, 15806, 13705, 360, 29892, 306, 674, 1603, 2326, 29876, 29871, 29941, 3291, 29892, 607, 338, 6133, 1135, 278, 29871, 29906, 3291, 306, 723, 679, 565, 306, 12784, 13705, 360, 6142, 29889, 7857, 29892, 306, 4658, 393, 13705, 315, 338, 278, 1900, 7348, 29892, 408, 372, 16688, 278, 9939, 7037, 5146, 2696, 322, 6260, 7093, 278, 12045, 310, 19035, 3291, 565, 590, 5614, 13148, 3060, 15806, 263, 1422, 13705, 29889, 13, 2528, 17658, 29892, 491, 23906, 13705, 315, 29892, 306, 626, 884, 7182, 292, 304, 590, 5614, 13148, 393, 306, 626, 19355, 304, 263, 901, 5177, 635, 19780, 2948, 29892, 607, 1033, 9949, 1009, 10608, 322, 3275, 304, 263, 14002, 594, 3385, 310, 13705, 315, 29889, 910, 723, 1121, 297, 263, 2253, 21957, 363, 1716, 14582, 322, 278, 5177, 29889, 2]\n",
      "labels:\n",
      "\n",
      "I would choose strategy C.\n",
      "My reasoning is that, since both companies are industry leaders, it is likely that both will want to adopt a policy that benefits their own company the most. In this case, strategy C provides the highest payoff for both companies, with a joint payoff of 10 points.\n",
      "If my coplayer chooses strategy D, I will still earn 3 points, which is higher than the 2 points I would get if I chose strategy D myself. Therefore, I believe that strategy C is the best choice, as it offers the highest potential payoff and minimizes the risk of losing points if my coplayer chooses a different strategy.\n",
      "Additionally, by choosing strategy C, I am also signaling to my coplayer that I am committed to a more environmentally friendly approach, which could influence their decision and lead to a joint adoption of strategy C. This would result in a better outcome for both companies and the environment.</s>\n",
      "Running tokenizer on dataset:   0%|             | 0/6000 [00:00<?, ? examples/s][INFO|configuration_utils.py:726] 2024-04-22 11:58:36,767 >> loading configuration file config.json from cache at /home/lora.n/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-22 11:58:36,768 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "model.safetensors.index.json: 100%|█████████| 26.8k/26.8k [00:00<00:00, 156MB/s]\n",
      "[INFO|modeling_utils.py:3283] 2024-04-22 11:58:37,166 >> loading weights file model.safetensors from cache at /home/lora.n/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/model.safetensors.index.json\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 10.5M/9.98G [00:00<02:02, 81.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 21.0M/9.98G [00:00<02:04, 79.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 31.5M/9.98G [00:00<02:03, 80.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 52.4M/9.98G [00:00<01:38, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 73.4M/9.98G [00:00<01:24, 117MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 94.4M/9.98G [00:00<01:15, 130MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|      | 115M/9.98G [00:00<01:12, 136MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|      | 136M/9.98G [00:01<01:08, 144MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|      | 157M/9.98G [00:01<01:06, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|      | 178M/9.98G [00:01<01:03, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|      | 199M/9.98G [00:01<01:02, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|▏     | 220M/9.98G [00:01<01:02, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|▏     | 241M/9.98G [00:01<01:01, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏     | 262M/9.98G [00:01<01:00, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏     | 283M/9.98G [00:02<01:02, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏     | 304M/9.98G [00:02<01:01, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏     | 325M/9.98G [00:02<01:01, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏     | 346M/9.98G [00:02<01:00, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏     | 367M/9.98G [00:02<01:03, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏     | 388M/9.98G [00:02<01:02, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏     | 409M/9.98G [00:02<01:01, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▎     | 430M/9.98G [00:02<01:02, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎     | 451M/9.98G [00:03<01:01, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎     | 472M/9.98G [00:03<01:00, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎     | 493M/9.98G [00:03<00:59, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎     | 514M/9.98G [00:03<00:58, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎     | 535M/9.98G [00:03<01:00, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎     | 556M/9.98G [00:03<00:58, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎     | 577M/9.98G [00:03<00:58, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎     | 598M/9.98G [00:04<01:00, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎     | 619M/9.98G [00:04<00:58, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▍     | 640M/9.98G [00:04<00:56, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 661M/9.98G [00:04<00:56, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 682M/9.98G [00:04<00:57, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 703M/9.98G [00:04<00:56, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 724M/9.98G [00:04<00:56, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 744M/9.98G [00:04<00:55, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍     | 765M/9.98G [00:05<00:55, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍     | 786M/9.98G [00:05<00:55, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍     | 807M/9.98G [00:05<00:55, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍     | 828M/9.98G [00:05<00:54, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 849M/9.98G [00:05<00:54, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 870M/9.98G [00:05<00:54, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 891M/9.98G [00:05<00:54, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 912M/9.98G [00:05<00:53, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 933M/9.98G [00:06<00:54, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌     | 954M/9.98G [00:06<00:54, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌     | 975M/9.98G [00:06<00:54, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌     | 996M/9.98G [00:06<00:54, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 1.02G/9.98G [00:06<00:54, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 1.04G/9.98G [00:06<00:55, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 1.06G/9.98G [00:06<00:54, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 1.08G/9.98G [00:06<00:57, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 1.10G/9.98G [00:07<00:54, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 1.12G/9.98G [00:07<00:54, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 1.14G/9.98G [00:07<00:55, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 1.16G/9.98G [00:07<00:55, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 1.18G/9.98G [00:07<00:55, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 1.21G/9.98G [00:07<00:54, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 1.23G/9.98G [00:07<00:55, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 1.25G/9.98G [00:07<00:54, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 1.27G/9.98G [00:08<00:53, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 1.29G/9.98G [00:08<00:54, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 1.31G/9.98G [00:08<00:52, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 1.33G/9.98G [00:08<00:52, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.35G/9.98G [00:08<00:53, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.37G/9.98G [00:08<00:53, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.39G/9.98G [00:08<00:52, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.42G/9.98G [00:09<00:51, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.44G/9.98G [00:09<00:54, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 1.46G/9.98G [00:09<00:55, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 1.48G/9.98G [00:09<00:55, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 1.50G/9.98G [00:09<00:55, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 1.52G/9.98G [00:09<00:56, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 1.54G/9.98G [00:09<00:54, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.56G/9.98G [00:09<00:52, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.58G/9.98G [00:10<00:53, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.60G/9.98G [00:10<00:53, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.63G/9.98G [00:10<00:51, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.65G/9.98G [00:10<00:50, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.67G/9.98G [00:10<00:50, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.69G/9.98G [00:10<00:56, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.71G/9.98G [00:10<00:58, 141MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.73G/9.98G [00:11<00:54, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 1.75G/9.98G [00:11<00:53, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 1.77G/9.98G [00:11<00:51, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 1.79G/9.98G [00:11<00:50, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 1.81G/9.98G [00:11<00:50, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 1.84G/9.98G [00:11<00:50, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.86G/9.98G [00:12<01:11, 113MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.88G/9.98G [00:12<01:02, 130MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.90G/9.98G [00:12<00:58, 137MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.92G/9.98G [00:12<00:55, 145MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.94G/9.98G [00:12<00:53, 150MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 1.96G/9.98G [00:12<00:52, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 1.98G/9.98G [00:12<00:50, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█    | 2.00G/9.98G [00:12<00:50, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█    | 2.02G/9.98G [00:13<00:48, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█    | 2.04G/9.98G [00:13<00:48, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.07G/9.98G [00:13<00:47, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.09G/9.98G [00:13<00:47, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.11G/9.98G [00:13<00:47, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.13G/9.98G [00:13<00:47, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.15G/9.98G [00:13<00:46, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.17G/9.98G [00:13<00:47, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.19G/9.98G [00:14<00:47, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.21G/9.98G [00:14<00:47, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.23G/9.98G [00:14<00:47, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.25G/9.98G [00:14<00:47, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.28G/9.98G [00:14<00:47, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.30G/9.98G [00:14<00:47, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.32G/9.98G [00:14<00:46, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.34G/9.98G [00:14<00:46, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.36G/9.98G [00:15<00:47, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.38G/9.98G [00:15<00:46, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.40G/9.98G [00:15<00:46, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.42G/9.98G [00:15<00:44, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.44G/9.98G [00:15<00:44, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▏   | 2.46G/9.98G [00:15<00:46, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▏   | 2.49G/9.98G [00:15<00:46, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▎   | 2.51G/9.98G [00:15<00:45, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▎   | 2.53G/9.98G [00:16<00:46, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.55G/9.98G [00:16<00:44, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.57G/9.98G [00:16<00:44, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.59G/9.98G [00:16<00:44, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.61G/9.98G [00:16<00:44, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.63G/9.98G [00:16<00:45, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.65G/9.98G [00:16<00:44, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.67G/9.98G [00:16<00:43, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.69G/9.98G [00:17<00:43, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.72G/9.98G [00:17<00:42, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.74G/9.98G [00:17<00:44, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.76G/9.98G [00:17<00:44, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.78G/9.98G [00:17<00:45, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.80G/9.98G [00:17<00:44, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.82G/9.98G [00:17<00:45, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.84G/9.98G [00:18<00:44, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.86G/9.98G [00:18<00:44, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.88G/9.98G [00:18<00:43, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.90G/9.98G [00:18<00:42, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.93G/9.98G [00:18<00:42, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▍   | 2.95G/9.98G [00:18<00:41, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▍   | 2.97G/9.98G [00:18<00:42, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▍   | 2.99G/9.98G [00:18<00:41, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▌   | 3.01G/9.98G [00:19<00:42, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▌   | 3.03G/9.98G [00:19<00:43, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.05G/9.98G [00:19<00:42, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.07G/9.98G [00:19<00:42, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.09G/9.98G [00:19<00:42, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.11G/9.98G [00:19<00:41, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.14G/9.98G [00:19<00:42, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.16G/9.98G [00:19<00:43, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.18G/9.98G [00:20<00:42, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.20G/9.98G [00:20<00:42, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.22G/9.98G [00:20<00:41, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.24G/9.98G [00:20<00:58, 115MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.26G/9.98G [00:20<00:51, 130MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.28G/9.98G [00:20<00:48, 138MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.30G/9.98G [00:21<00:45, 146MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.32G/9.98G [00:21<00:44, 150MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.34G/9.98G [00:21<00:42, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.37G/9.98G [00:21<00:58, 112MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.39G/9.98G [00:21<00:52, 125MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.41G/9.98G [00:21<00:50, 131MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.43G/9.98G [00:21<00:46, 140MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▋   | 3.45G/9.98G [00:22<00:44, 147MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▋   | 3.47G/9.98G [00:22<00:43, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▋   | 3.49G/9.98G [00:22<00:41, 155MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  17%|▎ | 1000/6000 [00:23<01:57, 42.58 examples/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▊   | 3.53G/9.98G [00:22<00:57, 112MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.55G/9.98G [00:22<00:53, 119MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.58G/9.98G [00:23<00:49, 128MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.60G/9.98G [00:23<00:46, 138MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.62G/9.98G [00:23<00:45, 140MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.64G/9.98G [00:23<00:43, 146MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▊   | 3.66G/9.98G [00:23<00:41, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▊   | 3.68G/9.98G [00:23<00:41, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▊   | 3.70G/9.98G [00:23<00:40, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▊   | 3.72G/9.98G [00:23<00:39, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.74G/9.98G [00:24<00:38, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.76G/9.98G [00:24<00:39, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.79G/9.98G [00:24<00:38, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.81G/9.98G [00:24<00:39, 155MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.83G/9.98G [00:24<00:39, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.85G/9.98G [00:24<00:38, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.87G/9.98G [00:24<00:39, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.89G/9.98G [00:25<00:38, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.91G/9.98G [00:25<00:38, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.93G/9.98G [00:25<00:38, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▉   | 3.95G/9.98G [00:25<00:37, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▉   | 3.97G/9.98G [00:25<00:36, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|██   | 4.00G/9.98G [00:25<00:36, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|██   | 4.02G/9.98G [00:25<00:37, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|██   | 4.04G/9.98G [00:25<00:37, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|██   | 4.06G/9.98G [00:26<00:36, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|██   | 4.08G/9.98G [00:26<00:37, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|██   | 4.10G/9.98G [00:26<00:37, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|██   | 4.12G/9.98G [00:26<00:36, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.14G/9.98G [00:26<00:53, 110MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.17G/9.98G [00:26<00:44, 131MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.19G/9.98G [00:27<00:42, 137MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.22G/9.98G [00:27<00:40, 144MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.24G/9.98G [00:27<00:38, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 4.26G/9.98G [00:27<00:36, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 4.28G/9.98G [00:27<00:35, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 4.30G/9.98G [00:27<00:34, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 4.32G/9.98G [00:27<00:34, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.34G/9.98G [00:28<00:34, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.36G/9.98G [00:28<00:34, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.38G/9.98G [00:28<00:34, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.40G/9.98G [00:28<00:35, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.42G/9.98G [00:28<00:34, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▏  | 4.45G/9.98G [00:28<00:34, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▏  | 4.47G/9.98G [00:28<00:33, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▏  | 4.49G/9.98G [00:28<00:32, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▎  | 4.51G/9.98G [00:29<00:32, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▎  | 4.53G/9.98G [00:29<00:31, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.55G/9.98G [00:29<00:31, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.57G/9.98G [00:29<00:31, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.59G/9.98G [00:29<00:32, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.61G/9.98G [00:29<00:32, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.63G/9.98G [00:29<00:31, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.66G/9.98G [00:29<00:31, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.68G/9.98G [00:30<00:31, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.70G/9.98G [00:30<00:33, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.72G/9.98G [00:30<00:32, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.74G/9.98G [00:30<00:31, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.76G/9.98G [00:30<00:30, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.78G/9.98G [00:30<00:31, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.80G/9.98G [00:30<00:30, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.82G/9.98G [00:30<00:29, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.84G/9.98G [00:31<00:30, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.87G/9.98G [00:31<00:30, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.89G/9.98G [00:31<00:30, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.91G/9.98G [00:31<00:30, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.93G/9.98G [00:31<00:29, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▍  | 4.95G/9.98G [00:31<00:30, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▍  | 4.97G/9.98G [00:31<00:30, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▌  | 4.99G/9.98G [00:31<00:29, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▌  | 5.01G/9.98G [00:32<00:31, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▌  | 5.03G/9.98G [00:32<00:29, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.05G/9.98G [00:32<00:30, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.08G/9.98G [00:32<00:30, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.10G/9.98G [00:32<00:30, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.12G/9.98G [00:32<00:29, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.14G/9.98G [00:32<00:28, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.16G/9.98G [00:32<00:28, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.18G/9.98G [00:33<00:28, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.20G/9.98G [00:33<00:27, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.22G/9.98G [00:33<00:28, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.24G/9.98G [00:33<00:28, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.26G/9.98G [00:33<00:28, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.28G/9.98G [00:33<00:28, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.31G/9.98G [00:33<00:27, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.33G/9.98G [00:33<00:27, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.35G/9.98G [00:34<00:28, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.37G/9.98G [00:34<00:29, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.39G/9.98G [00:34<00:31, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.41G/9.98G [00:34<00:31, 143MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.43G/9.98G [00:34<00:32, 141MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▋  | 5.45G/9.98G [00:34<00:33, 137MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▋  | 5.47G/9.98G [00:34<00:31, 143MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▊  | 5.49G/9.98G [00:35<00:30, 147MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▊  | 5.52G/9.98G [00:35<00:30, 146MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▊  | 5.54G/9.98G [00:35<00:39, 112MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▊  | 5.56G/9.98G [00:35<00:34, 126MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▊  | 5.58G/9.98G [00:35<00:32, 134MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▊  | 5.60G/9.98G [00:35<00:31, 138MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▊  | 5.62G/9.98G [00:36<00:31, 139MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.64G/9.98G [00:36<00:29, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.66G/9.98G [00:36<00:28, 150MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.68G/9.98G [00:36<00:28, 149MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.70G/9.98G [00:36<00:28, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.73G/9.98G [00:36<00:28, 149MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.75G/9.98G [00:36<00:27, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.77G/9.98G [00:37<00:28, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.79G/9.98G [00:37<00:27, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.81G/9.98G [00:37<00:27, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.83G/9.98G [00:37<00:27, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.85G/9.98G [00:37<00:27, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.87G/9.98G [00:37<00:27, 149MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  17%|▎ | 1000/6000 [00:38<01:56, 42.78 examples/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.91G/9.98G [00:38<00:26, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.93G/9.98G [00:38<00:26, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▉  | 5.96G/9.98G [00:38<00:26, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▉  | 5.98G/9.98G [00:38<00:28, 142MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|███  | 6.00G/9.98G [00:38<00:26, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|███  | 6.02G/9.98G [00:38<00:26, 150MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|███  | 6.04G/9.98G [00:38<00:24, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|███  | 6.06G/9.98G [00:38<00:24, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|███  | 6.08G/9.98G [00:39<00:24, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|███  | 6.10G/9.98G [00:39<00:24, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|███  | 6.12G/9.98G [00:39<00:23, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.14G/9.98G [00:39<00:22, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.17G/9.98G [00:39<00:22, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.19G/9.98G [00:39<00:22, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.21G/9.98G [00:39<00:22, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.23G/9.98G [00:39<00:22, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|███▏ | 6.25G/9.98G [00:40<00:21, 171MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|███▏ | 6.27G/9.98G [00:40<00:22, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|███▏ | 6.29G/9.98G [00:40<00:22, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|███▏ | 6.31G/9.98G [00:40<00:22, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|███▏ | 6.33G/9.98G [00:40<00:21, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.35G/9.98G [00:40<00:22, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.38G/9.98G [00:40<00:21, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.40G/9.98G [00:40<00:21, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.42G/9.98G [00:41<00:21, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|███▏ | 6.44G/9.98G [00:41<00:21, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|███▏ | 6.46G/9.98G [00:41<00:21, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|███▏ | 6.48G/9.98G [00:41<00:21, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|███▎ | 6.50G/9.98G [00:41<00:21, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|███▎ | 6.52G/9.98G [00:41<00:21, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.54G/9.98G [00:41<00:20, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.56G/9.98G [00:42<00:21, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.59G/9.98G [00:42<00:21, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.61G/9.98G [00:42<00:21, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.63G/9.98G [00:42<00:20, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.65G/9.98G [00:42<00:20, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.67G/9.98G [00:42<00:20, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.69G/9.98G [00:42<00:20, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.71G/9.98G [00:42<00:20, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.73G/9.98G [00:43<00:19, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 6.75G/9.98G [00:43<00:19, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 6.77G/9.98G [00:43<00:19, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 6.79G/9.98G [00:43<00:19, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 6.82G/9.98G [00:43<00:19, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.84G/9.98G [00:43<00:19, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.86G/9.98G [00:43<00:26, 119MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.88G/9.98G [00:44<00:24, 127MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.90G/9.98G [00:44<00:23, 130MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.92G/9.98G [00:44<00:22, 139MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▍ | 6.94G/9.98G [00:44<00:20, 145MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▍ | 6.96G/9.98G [00:44<00:20, 147MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▍ | 6.98G/9.98G [00:44<00:20, 145MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▌ | 7.00G/9.98G [00:44<00:20, 142MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  33%|▋ | 2000/6000 [00:46<01:31, 43.54 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  33%|▋ | 2000/6000 [00:46<01:32, 43.37 examples/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███▌ | 7.07G/9.98G [00:45<00:19, 150MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  33%|▋ | 2000/6000 [00:46<01:32, 43.11 examples/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███▌ | 7.11G/9.98G [00:45<00:18, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███▌ | 7.13G/9.98G [00:45<00:18, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███▌ | 7.15G/9.98G [00:45<00:18, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███▌ | 7.17G/9.98G [00:46<00:24, 113MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███▌ | 7.19G/9.98G [00:46<00:22, 126MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███▌ | 7.21G/9.98G [00:46<00:20, 134MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.24G/9.98G [00:46<00:19, 141MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.26G/9.98G [00:46<00:18, 143MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.28G/9.98G [00:46<00:17, 150MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.30G/9.98G [00:47<00:17, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.32G/9.98G [00:47<00:16, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.34G/9.98G [00:47<00:17, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.36G/9.98G [00:47<00:16, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.38G/9.98G [00:47<00:16, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.40G/9.98G [00:47<00:16, 158MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.42G/9.98G [00:47<00:15, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▋ | 7.44G/9.98G [00:47<00:15, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▋ | 7.47G/9.98G [00:48<00:15, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▊ | 7.49G/9.98G [00:48<00:15, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▊ | 7.51G/9.98G [00:48<00:14, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▊ | 7.53G/9.98G [00:48<00:14, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 7.55G/9.98G [00:48<00:14, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 7.57G/9.98G [00:48<00:14, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 7.59G/9.98G [00:48<00:14, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 7.61G/9.98G [00:48<00:14, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.63G/9.98G [00:49<00:14, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.65G/9.98G [00:49<00:14, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.68G/9.98G [00:49<00:14, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.70G/9.98G [00:49<00:13, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.72G/9.98G [00:49<00:13, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.74G/9.98G [00:49<00:13, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.76G/9.98G [00:49<00:13, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.78G/9.98G [00:49<00:13, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.80G/9.98G [00:50<00:13, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.82G/9.98G [00:50<00:13, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.84G/9.98G [00:50<00:13, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.86G/9.98G [00:50<00:12, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.89G/9.98G [00:50<00:12, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.91G/9.98G [00:50<00:12, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.93G/9.98G [00:50<00:12, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▉ | 7.95G/9.98G [00:51<00:12, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▉ | 7.97G/9.98G [00:51<00:12, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|████ | 7.99G/9.98G [00:51<00:12, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|████ | 8.01G/9.98G [00:51<00:12, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.03G/9.98G [00:51<00:12, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.05G/9.98G [00:51<00:12, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.07G/9.98G [00:51<00:11, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.10G/9.98G [00:51<00:11, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.12G/9.98G [00:52<00:11, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.14G/9.98G [00:52<00:11, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.16G/9.98G [00:52<00:16, 111MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.18G/9.98G [00:52<00:14, 124MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.20G/9.98G [00:52<00:13, 131MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.22G/9.98G [00:52<00:12, 138MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.24G/9.98G [00:53<00:12, 140MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.26G/9.98G [00:53<00:11, 143MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.28G/9.98G [00:53<00:11, 141MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.30G/9.98G [00:53<00:11, 144MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.33G/9.98G [00:53<00:11, 146MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.35G/9.98G [00:53<00:11, 144MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.37G/9.98G [00:53<00:11, 146MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.39G/9.98G [00:54<00:10, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.41G/9.98G [00:54<00:10, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▏| 8.43G/9.98G [00:54<00:10, 149MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▏| 8.45G/9.98G [00:54<00:10, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▏| 8.47G/9.98G [00:54<00:10, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▎| 8.49G/9.98G [00:54<00:09, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▎| 8.51G/9.98G [00:54<00:09, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.54G/9.98G [00:55<00:09, 150MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.56G/9.98G [00:55<00:09, 147MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.58G/9.98G [00:55<00:09, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.60G/9.98G [00:55<00:09, 150MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.62G/9.98G [00:55<00:09, 150MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.64G/9.98G [00:55<00:08, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.66G/9.98G [00:55<00:08, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.68G/9.98G [00:55<00:08, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.70G/9.98G [00:56<00:08, 147MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.72G/9.98G [00:56<00:11, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.75G/9.98G [00:56<00:13, 89.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.77G/9.98G [00:57<00:14, 80.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|███▌| 8.79G/9.98G [00:57<00:12, 92.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████▍| 8.81G/9.98G [00:57<00:10, 109MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████▍| 8.83G/9.98G [00:57<00:09, 120MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 8.85G/9.98G [00:57<00:08, 130MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 8.87G/9.98G [00:57<00:08, 137MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  33%|▋ | 2000/6000 [00:58<01:32, 43.37 examples/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 8.91G/9.98G [00:58<00:07, 148MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████▍| 8.93G/9.98G [00:58<00:06, 153MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████▍| 8.95G/9.98G [00:58<00:06, 151MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████▍| 8.98G/9.98G [00:58<00:06, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████▌| 9.00G/9.98G [00:58<00:06, 157MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████▌| 9.02G/9.98G [00:58<00:05, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.04G/9.98G [00:58<00:05, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.06G/9.98G [00:58<00:05, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.08G/9.98G [00:59<00:05, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.10G/9.98G [00:59<00:05, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.12G/9.98G [00:59<00:05, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.14G/9.98G [00:59<00:05, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.16G/9.98G [00:59<00:04, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.19G/9.98G [00:59<00:04, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.21G/9.98G [00:59<00:04, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.23G/9.98G [00:59<00:04, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|████▋| 9.25G/9.98G [01:00<00:04, 161MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|████▋| 9.27G/9.98G [01:00<00:04, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|████▋| 9.29G/9.98G [01:00<00:04, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|████▋| 9.31G/9.98G [01:00<00:04, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.33G/9.98G [01:00<00:03, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.35G/9.98G [01:00<00:03, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.37G/9.98G [01:00<00:03, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.40G/9.98G [01:00<00:03, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.42G/9.98G [01:01<00:03, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▋| 9.44G/9.98G [01:01<00:03, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▋| 9.46G/9.98G [01:01<00:03, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▊| 9.48G/9.98G [01:01<00:02, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▊| 9.50G/9.98G [01:01<00:02, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▊| 9.52G/9.98G [01:01<00:02, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|████▊| 9.54G/9.98G [01:01<00:02, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|████▊| 9.56G/9.98G [01:01<00:02, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|████▊| 9.58G/9.98G [01:02<00:02, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|████▊| 9.60G/9.98G [01:02<00:02, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|████▊| 9.63G/9.98G [01:02<00:02, 160MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 9.65G/9.98G [01:02<00:01, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 9.67G/9.98G [01:02<00:01, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 9.69G/9.98G [01:02<00:01, 169MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 9.71G/9.98G [01:02<00:01, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.73G/9.98G [01:02<00:01, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.75G/9.98G [01:03<00:01, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.77G/9.98G [01:03<00:01, 164MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.79G/9.98G [01:03<00:01, 167MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.81G/9.98G [01:03<00:00, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.84G/9.98G [01:03<00:00, 165MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.86G/9.98G [01:03<00:00, 168MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.88G/9.98G [01:03<00:00, 166MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.90G/9.98G [01:04<00:00, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.92G/9.98G [01:04<00:00, 163MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|████▉| 9.94G/9.98G [01:04<00:00, 162MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|█████| 9.98G/9.98G [01:04<00:00, 155MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  50%|█ | 3000/6000 [01:09<01:09, 43.14 examples/s]\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|     | 21.0M/3.50G [00:00<00:25, 138MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|     | 41.9M/3.50G [00:00<00:23, 146MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  50%|█ | 3000/6000 [01:09<01:10, 42.86 examples/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|     | 83.9M/3.50G [00:00<00:23, 148MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|▏     | 105M/3.50G [00:00<00:21, 157MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏     | 126M/3.50G [00:00<00:21, 161MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▎     | 147M/3.50G [00:00<00:20, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎     | 168M/3.50G [00:01<00:21, 156MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎     | 189M/3.50G [00:01<00:21, 157MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎     | 210M/3.50G [00:01<00:20, 159MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▍     | 231M/3.50G [00:01<00:19, 164MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▍     | 252M/3.50G [00:01<00:19, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍     | 273M/3.50G [00:01<00:20, 160MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▌     | 294M/3.50G [00:01<00:20, 155MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▌     | 315M/3.50G [00:02<00:19, 159MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▌     | 336M/3.50G [00:02<00:20, 152MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▌     | 357M/3.50G [00:02<00:20, 152MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▋     | 377M/3.50G [00:02<00:20, 152MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▋     | 398M/3.50G [00:02<00:20, 155MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▋     | 419M/3.50G [00:02<00:19, 155MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▊     | 440M/3.50G [00:03<00:27, 112MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▊     | 461M/3.50G [00:03<00:25, 119MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▊     | 482M/3.50G [00:03<00:23, 130MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▊     | 503M/3.50G [00:03<00:20, 143MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▉     | 524M/3.50G [00:03<00:20, 142MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▉     | 545M/3.50G [00:03<00:29, 101MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▉     | 566M/3.50G [00:04<00:25, 115MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|█     | 587M/3.50G [00:04<00:22, 130MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|█     | 608M/3.50G [00:04<00:21, 137MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|█     | 629M/3.50G [00:04<00:19, 145MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|█     | 650M/3.50G [00:04<00:19, 148MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|█▏    | 671M/3.50G [00:04<00:18, 155MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█▏    | 692M/3.50G [00:04<00:17, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█▏    | 713M/3.50G [00:04<00:17, 159MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█▎    | 734M/3.50G [00:05<00:16, 164MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█▎    | 755M/3.50G [00:05<00:16, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█▎    | 776M/3.50G [00:05<00:16, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▎    | 797M/3.50G [00:05<00:16, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▍    | 818M/3.50G [00:05<00:16, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▍    | 839M/3.50G [00:05<00:16, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▍    | 860M/3.50G [00:05<00:15, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▌    | 881M/3.50G [00:05<00:15, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▌    | 902M/3.50G [00:06<00:15, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▌    | 923M/3.50G [00:06<00:15, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▌    | 944M/3.50G [00:06<00:15, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▋    | 965M/3.50G [00:06<00:15, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▋    | 986M/3.50G [00:06<00:15, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 1.01G/3.50G [00:06<00:15, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 1.03G/3.50G [00:06<00:15, 159MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▍   | 1.05G/3.50G [00:06<00:15, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 1.07G/3.50G [00:07<00:14, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 1.09G/3.50G [00:07<00:14, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 1.11G/3.50G [00:07<00:15, 159MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 1.13G/3.50G [00:07<00:15, 151MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 1.15G/3.50G [00:07<00:16, 144MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 1.17G/3.50G [00:07<00:16, 145MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 1.20G/3.50G [00:07<00:16, 141MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▋   | 1.22G/3.50G [00:08<00:15, 144MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▊   | 1.24G/3.50G [00:08<00:15, 145MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▊   | 1.26G/3.50G [00:08<00:14, 150MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 1.28G/3.50G [00:08<00:14, 153MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 1.30G/3.50G [00:08<00:14, 157MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▉   | 1.32G/3.50G [00:08<00:13, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▉   | 1.34G/3.50G [00:08<00:13, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▉   | 1.36G/3.50G [00:09<00:18, 113MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▉   | 1.38G/3.50G [00:09<00:17, 122MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|██   | 1.41G/3.50G [00:09<00:16, 128MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|██   | 1.43G/3.50G [00:09<00:15, 131MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|██   | 1.45G/3.50G [00:09<00:14, 138MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|██   | 1.47G/3.50G [00:09<00:14, 141MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|██▏  | 1.49G/3.50G [00:10<00:14, 142MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|██▏  | 1.51G/3.50G [00:10<00:19, 104MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|██▏  | 1.53G/3.50G [00:10<00:17, 113MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|██▏  | 1.55G/3.50G [00:10<00:15, 122MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|██▏  | 1.57G/3.50G [00:10<00:14, 130MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|██▎  | 1.59G/3.50G [00:10<00:14, 135MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|██▎  | 1.61G/3.50G [00:11<00:17, 105MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|██▎  | 1.64G/3.50G [00:11<00:16, 116MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|██▎  | 1.66G/3.50G [00:11<00:15, 122MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|██▍  | 1.68G/3.50G [00:11<00:14, 129MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|██▍  | 1.70G/3.50G [00:11<00:13, 135MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|██▍  | 1.72G/3.50G [00:11<00:12, 137MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██▍  | 1.74G/3.50G [00:12<00:12, 140MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██▌  | 1.76G/3.50G [00:12<00:11, 146MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██▌  | 1.78G/3.50G [00:12<00:11, 146MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██▌  | 1.80G/3.50G [00:12<00:11, 148MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██▌  | 1.82G/3.50G [00:12<00:15, 109MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██▋  | 1.85G/3.50G [00:12<00:14, 118MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██▋  | 1.87G/3.50G [00:13<00:13, 124MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▋  | 1.89G/3.50G [00:13<00:12, 132MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▋  | 1.91G/3.50G [00:13<00:11, 135MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▊  | 1.93G/3.50G [00:13<00:11, 137MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▊  | 1.95G/3.50G [00:13<00:10, 145MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▊  | 1.97G/3.50G [00:13<00:10, 146MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▊  | 1.99G/3.50G [00:13<00:10, 150MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▉  | 2.01G/3.50G [00:14<00:09, 154MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▉  | 2.03G/3.50G [00:14<00:09, 153MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▉  | 2.06G/3.50G [00:14<00:09, 155MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▉  | 2.08G/3.50G [00:14<00:09, 155MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▉  | 2.10G/3.50G [00:14<00:08, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|███  | 2.12G/3.50G [00:14<00:08, 160MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|███  | 2.14G/3.50G [00:14<00:08, 164MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|███  | 2.16G/3.50G [00:14<00:08, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|███  | 2.18G/3.50G [00:15<00:08, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|███▏ | 2.20G/3.50G [00:15<00:07, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|███▏ | 2.22G/3.50G [00:15<00:07, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|███▏ | 2.24G/3.50G [00:15<00:07, 164MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|███▏ | 2.26G/3.50G [00:15<00:07, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|███▎ | 2.29G/3.50G [00:15<00:07, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|███▎ | 2.31G/3.50G [00:15<00:07, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|███▎ | 2.33G/3.50G [00:15<00:07, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|███▎ | 2.35G/3.50G [00:16<00:10, 115MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|███▍ | 2.37G/3.50G [00:16<00:08, 130MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|███▍ | 2.39G/3.50G [00:16<00:08, 136MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|███▍ | 2.41G/3.50G [00:16<00:07, 140MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|███▍ | 2.43G/3.50G [00:16<00:07, 148MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|███▌ | 2.45G/3.50G [00:16<00:07, 148MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|███▌ | 2.47G/3.50G [00:17<00:06, 150MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|███▌ | 2.50G/3.50G [00:17<00:06, 154MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|███▌ | 2.52G/3.50G [00:17<00:06, 157MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|███▌ | 2.54G/3.50G [00:17<00:06, 153MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|███▋ | 2.56G/3.50G [00:17<00:05, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|███▋ | 2.58G/3.50G [00:17<00:05, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|███▋ | 2.60G/3.50G [00:17<00:05, 160MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|███▋ | 2.62G/3.50G [00:17<00:05, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|███▊ | 2.64G/3.50G [00:18<00:05, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███▊ | 2.66G/3.50G [00:18<00:05, 164MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███▊ | 2.68G/3.50G [00:18<00:04, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███▊ | 2.71G/3.50G [00:18<00:04, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███▉ | 2.73G/3.50G [00:18<00:04, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███▉ | 2.75G/3.50G [00:18<00:04, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▉ | 2.77G/3.50G [00:18<00:04, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███▉ | 2.79G/3.50G [00:18<00:04, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|████ | 2.81G/3.50G [00:19<00:04, 165MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  50%|█ | 3000/6000 [01:28<01:09, 43.19 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  50%|█ | 3000/6000 [01:28<01:09, 43.14 examples/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|████ | 2.87G/3.50G [00:19<00:03, 167MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|████▏| 2.89G/3.50G [00:19<00:03, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|████▏| 2.92G/3.50G [00:19<00:03, 172MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|████▏| 2.94G/3.50G [00:19<00:03, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|████▏| 2.96G/3.50G [00:19<00:03, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|████▎| 2.98G/3.50G [00:20<00:03, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|████▎| 3.00G/3.50G [00:20<00:02, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|████▎| 3.02G/3.50G [00:20<00:02, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|████▎| 3.04G/3.50G [00:20<00:02, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|████▎| 3.06G/3.50G [00:20<00:02, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|████▍| 3.08G/3.50G [00:20<00:02, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|████▍| 3.10G/3.50G [00:20<00:02, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|████▍| 3.12G/3.50G [00:21<00:02, 160MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|████▍| 3.15G/3.50G [00:21<00:02, 150MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|████▌| 3.17G/3.50G [00:21<00:02, 154MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  67%|█▎| 4000/6000 [01:31<00:45, 44.39 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  67%|█▎| 4000/6000 [01:31<00:45, 44.34 examples/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|████▌| 3.23G/3.50G [00:21<00:01, 158MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|████▋| 3.25G/3.50G [00:21<00:01, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|████▋| 3.27G/3.50G [00:21<00:01, 161MB/s]\u001b[A\n",
      "Running tokenizer on dataset:  67%|█▎| 4000/6000 [01:31<00:45, 44.08 examples/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|████▋| 3.31G/3.50G [00:22<00:01, 164MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|████▊| 3.33G/3.50G [00:22<00:01, 159MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|████▊| 3.36G/3.50G [00:22<00:00, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|████▊| 3.38G/3.50G [00:22<00:00, 168MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|████▊| 3.40G/3.50G [00:22<00:00, 163MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|████▉| 3.42G/3.50G [00:22<00:00, 166MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|████▉| 3.44G/3.50G [00:22<00:00, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|████▉| 3.46G/3.50G [00:23<00:00, 169MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|████▉| 3.48G/3.50G [00:23<00:00, 165MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|█████| 3.50G/3.50G [00:23<00:00, 150MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [01:35<00:00, 47.68s/it]\n",
      "[INFO|modeling_utils.py:1417] 2024-04-22 12:00:12,550 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|modeling_utils.py:3392] 2024-04-22 12:00:12,550 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "[INFO|configuration_utils.py:928] 2024-04-22 12:00:12,572 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset: 100%|██| 6000/6000 [02:11<00:00, 45.61 examples/s]\n",
      "Running tokenizer on dataset: 100%|██| 6000/6000 [02:11<00:00, 45.60 examples/s]\n",
      "Running tokenizer on dataset: 100%|██| 6000/6000 [02:12<00:00, 45.42 examples/s]\n",
      "[2024-04-22 12:00:50,326] [INFO] [partition_parameters.py:349:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:25<00:00, 12.76s/it]\n",
      "generation_config.json: 100%|██████████████████| 188/188 [00:00<00:00, 1.19MB/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:25<00:00, 12.83s/it]\n",
      "04/22/2024 12:01:16 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "04/22/2024 12:01:16 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:25<00:00, 12.89s/it]\n",
      "04/22/2024 12:01:16 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "04/22/2024 12:01:16 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "04/22/2024 12:01:16 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "04/22/2024 12:01:16 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "Loading checkpoint shards:  50%|█████████         | 1/2 [00:32<00:32, 32.30s/it]04/22/2024 12:01:33 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622\n",
      "04/22/2024 12:01:33 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622\n",
      "04/22/2024 12:01:33 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622\n",
      "/home/lora.n/.conda/envs/deepseek/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/lora.n/.conda/envs/deepseek/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/lora.n/.conda/envs/deepseek/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:43<00:00, 21.98s/it]\n",
      "[INFO|modeling_utils.py:4024] 2024-04-22 12:01:34,458 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4032] 2024-04-22 12:01:34,458 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-04-22 12:01:34,566 >> loading configuration file generation_config.json from cache at /home/lora.n/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-04-22 12:01:34,567 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "04/22/2024 12:01:34 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "04/22/2024 12:01:34 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "04/22/2024 12:01:35 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622\n",
      "/home/lora.n/.conda/envs/deepseek/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:604] 2024-04-22 12:01:35,098 >> Using auto half precision backend\n",
      "[2024-04-22 12:01:35,974] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-04-22 12:01:36,043] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-04-22 12:01:36,050] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-04-22 12:01:36,053] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-04-22 12:01:36,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2024-04-22 12:01:36,085] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2024-04-22 12:01:36,085] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-04-22 12:01:36,085] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-04-22 12:01:36,820] [INFO] [utils.py:791:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-04-22 12:01:36,833] [INFO] [utils.py:792:see_memory_usage] MA 3.23 GB         Max_MA 3.71 GB         CA 3.5 GB         Max_CA 4 GB \n",
      "[2024-04-22 12:01:36,833] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.38 GB, percent = 6.6%\n",
      "[2024-04-22 12:01:36,850] [INFO] [stage3.py:128:__init__] Reduce bucket size 16777216\n",
      "[2024-04-22 12:01:36,851] [INFO] [stage3.py:129:__init__] Prefetch bucket size 15099494\n",
      "[2024-04-22 12:01:37,595] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-04-22 12:01:37,601] [INFO] [utils.py:792:see_memory_usage] MA 3.23 GB         Max_MA 3.23 GB         CA 3.5 GB         Max_CA 3 GB \n",
      "[2024-04-22 12:01:37,601] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.38 GB, percent = 6.6%\n",
      "Parameter Offload: Total persistent parameters: 4460544 in 193 params\n",
      "[2024-04-22 12:01:38,650] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-04-22 12:01:38,668] [INFO] [utils.py:792:see_memory_usage] MA 3.23 GB         Max_MA 3.23 GB         CA 3.5 GB         Max_CA 3 GB \n",
      "[2024-04-22 12:01:38,669] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.38 GB, percent = 6.6%\n",
      "[2024-04-22 12:01:39,451] [INFO] [utils.py:791:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-04-22 12:01:39,452] [INFO] [utils.py:792:see_memory_usage] MA 3.23 GB         Max_MA 3.23 GB         CA 3.5 GB         Max_CA 3 GB \n",
      "[2024-04-22 12:01:39,452] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.38 GB, percent = 6.6%\n",
      "[2024-04-22 12:01:41,155] [INFO] [utils.py:791:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2024-04-22 12:01:41,179] [INFO] [utils.py:792:see_memory_usage] MA 3.23 GB         Max_MA 3.23 GB         CA 3.48 GB         Max_CA 3 GB \n",
      "[2024-04-22 12:01:41,179] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.38 GB, percent = 6.6%\n",
      "[2024-04-22 12:01:41,957] [INFO] [utils.py:791:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-04-22 12:01:41,958] [INFO] [utils.py:792:see_memory_usage] MA 3.23 GB         Max_MA 3.23 GB         CA 3.48 GB         Max_CA 3 GB \n",
      "[2024-04-22 12:01:41,958] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.38 GB, percent = 6.6%\n",
      "[2024-04-22 12:01:42,723] [INFO] [utils.py:791:see_memory_usage] After creating fp32 partitions\n",
      "[2024-04-22 12:01:42,738] [INFO] [utils.py:792:see_memory_usage] MA 3.23 GB         Max_MA 3.23 GB         CA 3.48 GB         Max_CA 3 GB \n",
      "[2024-04-22 12:01:42,738] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.38 GB, percent = 6.6%\n",
      "[2024-04-22 12:01:43,578] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-04-22 12:01:43,586] [INFO] [utils.py:792:see_memory_usage] MA 3.23 GB         Max_MA 3.23 GB         CA 3.48 GB         Max_CA 3 GB \n",
      "[2024-04-22 12:01:43,586] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.51 GB, percent = 6.7%\n",
      "[2024-04-22 12:01:44,680] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-04-22 12:01:44,686] [INFO] [utils.py:792:see_memory_usage] MA 3.24 GB         Max_MA 3.24 GB         CA 3.48 GB         Max_CA 3 GB \n",
      "[2024-04-22 12:01:44,691] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.55 GB, percent = 6.7%\n",
      "[2024-04-22 12:01:44,692] [INFO] [stage3.py:482:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2024-04-22 12:01:45,650] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-04-22 12:01:45,651] [INFO] [utils.py:792:see_memory_usage] MA 3.27 GB         Max_MA 3.27 GB         CA 3.51 GB         Max_CA 4 GB \n",
      "[2024-04-22 12:01:45,658] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.63 GB, percent = 6.7%\n",
      "[2024-04-22 12:01:45,659] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2024-04-22 12:01:45,659] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-04-22 12:01:45,659] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-04-22 12:01:45,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]\n",
      "[2024-04-22 12:01:45,661] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2ad1368fada0>\n",
      "[2024-04-22 12:01:45,668] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   fp16_auto_cast ............... False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   fp16_enabled ................. True\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-04-22 12:01:45,669] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 4\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   optimizer_name ............... None\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   optimizer_params ............. None\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   scheduler_name ............... None\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   scheduler_params ............. None\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   steps_per_print .............. inf\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   train_batch_size ............. 64\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  4\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-04-22 12:01:45,670] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-04-22 12:01:45,671] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-04-22 12:01:45,671] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-04-22 12:01:45,671] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True\n",
      "[2024-04-22 12:01:45,671] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-04-22 12:01:45,671] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-04-22 12:01:45,671] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-04-22 12:01:45,671] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3\n",
      "[2024-04-22 12:01:45,671] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\n",
      "}\n",
      "[INFO|trainer.py:1827] 2024-04-22 12:01:45,671 >> ***** Running training *****\n",
      "[INFO|trainer.py:1828] 2024-04-22 12:01:45,672 >>   Num examples = 6,000\n",
      "[INFO|trainer.py:1829] 2024-04-22 12:01:45,672 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1830] 2024-04-22 12:01:45,672 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1833] 2024-04-22 12:01:45,672 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:1834] 2024-04-22 12:01:45,672 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1835] 2024-04-22 12:01:45,672 >>   Total optimization steps = 279\n",
      "[INFO|trainer.py:1836] 2024-04-22 12:01:45,674 >>   Number of trainable parameters = 4,194,304\n",
      "{'loss': 0.8725, 'grad_norm': 0.46446388515436365, 'learning_rate': 4.984167757679458e-05, 'epoch': 0.11}\n",
      "{'loss': 0.7337, 'grad_norm': 0.3246637715860352, 'learning_rate': 4.936871558635346e-05, 'epoch': 0.21}\n",
      "{'loss': 0.6474, 'grad_norm': 0.29468978605872753, 'learning_rate': 4.858710446774951e-05, 'epoch': 0.32}\n",
      "{'loss': 0.5856, 'grad_norm': 0.3125500228373009, 'learning_rate': 4.750674394628687e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5442, 'grad_norm': 0.25926994965782113, 'learning_rate': 4.6141317645621e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5052, 'grad_norm': 0.2569351995766955, 'learning_rate': 4.45081197738023e-05, 'epoch': 0.64}\n",
      "{'loss': 0.4588, 'grad_norm': 0.2438216434408768, 'learning_rate': 4.262783607840199e-05, 'epoch': 0.75}\n",
      "{'loss': 0.4559, 'grad_norm': 0.2616089253647084, 'learning_rate': 4.052428184509762e-05, 'epoch': 0.85}\n",
      "{'loss': 0.4366, 'grad_norm': 0.2719947436501133, 'learning_rate': 3.822410025817406e-05, 'epoch': 0.96}\n",
      "{'loss': 0.4184, 'grad_norm': 0.2800286451842843, 'learning_rate': 3.575642494344365e-05, 'epoch': 1.07}\n",
      "{'loss': 0.4244, 'grad_norm': 0.3339809462575545, 'learning_rate': 3.315251096774737e-05, 'epoch': 1.17}\n",
      "{'loss': 0.4184, 'grad_norm': 0.2972277651412302, 'learning_rate': 3.0445338968721287e-05, 'epoch': 1.28}\n",
      "{'loss': 0.4045, 'grad_norm': 0.2987460585001557, 'learning_rate': 2.7669197428838972e-05, 'epoch': 1.39}\n",
      "{'loss': 0.4044, 'grad_norm': 0.3166704553574001, 'learning_rate': 2.485924838456086e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3946, 'grad_norm': 0.324319393332511, 'learning_rate': 2.2051082071228854e-05, 'epoch': 1.6}\n",
      "{'loss': 0.3977, 'grad_norm': 0.3697740636110134, 'learning_rate': 1.928026614448221e-05, 'epoch': 1.71}\n",
      "{'loss': 0.3959, 'grad_norm': 0.3433087824950884, 'learning_rate': 1.658189518766322e-05, 'epoch': 1.81}\n",
      "{'loss': 0.3937, 'grad_norm': 0.379113943813783, 'learning_rate': 1.399014621105914e-05, 'epoch': 1.92}\n",
      "{'loss': 0.3886, 'grad_norm': 0.3407832333059858, 'learning_rate': 1.1537845772935279e-05, 'epoch': 2.03}\n",
      "{'loss': 0.3756, 'grad_norm': 0.33870470292827976, 'learning_rate': 9.256054205114939e-06, 'epoch': 2.13}\n",
      "{'loss': 0.3878, 'grad_norm': 0.35455050560695944, 'learning_rate': 7.173672209219495e-06, 'epoch': 2.24}\n",
      "{'loss': 0.3827, 'grad_norm': 0.34069480671818303, 'learning_rate': 5.317074806339295e-06, 'epoch': 2.35}\n",
      "{'loss': 0.3862, 'grad_norm': 0.34448855176085075, 'learning_rate': 3.70977727645363e-06, 'epoch': 2.45}\n",
      "{'loss': 0.3835, 'grad_norm': 0.36964766952982536, 'learning_rate': 2.372137318741968e-06, 'epoch': 2.56}\n",
      "{'loss': 0.3841, 'grad_norm': 0.3243623051744644, 'learning_rate': 1.3210972051628328e-06, 'epoch': 2.67}\n",
      "{'loss': 0.3896, 'grad_norm': 0.380645721199869, 'learning_rate': 5.699691931296463e-07, 'epoch': 2.77}\n",
      "{'loss': 0.3699, 'grad_norm': 0.3340016289757529, 'learning_rate': 1.2826691520262114e-07, 'epoch': 2.88}\n",
      "100%|█████████████████████████████████████████| 279/279 [58:08<00:00, 12.41s/it][INFO|trainer.py:2085] 2024-04-22 12:59:54,487 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3488.8091, 'train_samples_per_second': 5.159, 'train_steps_per_second': 0.08, 'train_loss': 0.45403385589626954, 'epoch': 2.98}\n",
      "100%|█████████████████████████████████████████| 279/279 [58:08<00:00, 12.50s/it]\n",
      "[INFO|trainer.py:3057] 2024-04-22 13:00:37,623 >> Saving model checkpoint to llama2_lora\n",
      "[INFO|configuration_utils.py:726] 2024-04-22 13:00:38,053 >> loading configuration file config.json from cache at /home/lora.n/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-22 13:00:38,053 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2502] 2024-04-22 13:00:38,129 >> tokenizer config file saved in llama2_lora/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2511] 2024-04-22 13:00:38,133 >> Special tokens file saved in llama2_lora/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.98\n",
      "  train_loss               =      0.454\n",
      "  train_runtime            = 0:58:08.80\n",
      "  train_samples_per_second =      5.159\n",
      "  train_steps_per_second   =       0.08\n",
      "Figure saved at: llama2_lora/training_loss.png\n",
      "04/22/2024 13:00:42 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.\n",
      "[INFO|modelcard.py:450] 2024-04-22 13:00:42,630 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[2024-04-22 13:00:43,923] [INFO] [launch.py:347:main] Process 34549 exits successfully.\n",
      "[2024-04-22 13:00:44,924] [INFO] [launch.py:347:main] Process 34550 exits successfully.\n",
      "[2024-04-22 13:00:44,925] [INFO] [launch.py:347:main] Process 34551 exits successfully.\n",
      "[2024-04-22 13:00:45,926] [INFO] [launch.py:347:main] Process 34548 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --num_gpus 4 --master_port=9901 src/train_bash.py \\\n",
    "    --deepspeed \"./examples/deepspeed/ds_z3_config.json\" \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path 'meta-llama/Llama-2-7b-chat-hf' \\\n",
    "    --dataset factory_ready \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir \"llama2_lora\" \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 1000 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
