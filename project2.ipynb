{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1781055-5c53-4eb1-9235-beabecaebaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49e175-486b-4ffe-a5fc-58c0fe688eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU transformers accelerate einops langchain xformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee75c8c-a178-413b-bc60-f91af6c68176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae4153d-a31c-40e5-bc1f-4733170d56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "028be0d6-0374-4444-84a3-4840a7c5e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9239fc2-63f4-4c01-94d8-ce99a15f590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8bac17-6973-4c06-99b6-2304fa09d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU transformers accelerate einops langchain xformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ba0dc-faa9-4bd9-b133-e4af964c062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48a9c71-a769-4707-9ec6-e4b710fbfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import pickle\n",
    "import transformers\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "import time\n",
    "\n",
    "groq_auth = \"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\"\n",
    "model_id = 'meta-llama/Meta-Llama-3-8B'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561713ed-ff51-4f93-bc82-5f38d1efdd70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''model_id = 'meta-llama/Meta-Llama-3-8B'\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16,\n",
    "    load_in_8bit_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_auth\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c88f59e-74e0-4bbe-839f-0e2ca173e05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae026608ab14070ba402b34a8c1b91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''try:\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        #\"/scratch/lora.n/LLaMa2/config.py\",\n",
    "        #from_tf = True,\n",
    "        cache_dir='/scratch/lora.n/STS_Project',\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=hf_auth\n",
    "    )\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\", e)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04152952-0681-4629-a676-b038b29b7a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "'''model.eval()\n",
    "print(f\"Model loaded on {device}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5368365-df97-4e48-ac14-331582623e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "'''tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_auth\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a140f1ad-4253-406d-b187-20997c5a9337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[128000, 198, 35075, 25], [128000, 198, 14196, 4077]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9a440a4-44d8-4bcb-8e95-0134e32f8287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([128000,    198,  35075,     25], device='cuda:0'),\n",
       " tensor([128000,    198,  14196,   4077], device='cuda:0')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4a91aa4-9aa0-4cf2-9193-9f2c4dda098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.StopOnTokens at 0x2b81920d14e0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "stopping_criteria'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cac53aa9-1190-441a-8aac-b11ff2e7cf75",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not infer framework from class <class 'langchain_groq.chat_models.ChatGroq'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_groq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatGroq(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-8b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m, groq_api_key\u001b[38;5;241m=\u001b[39mgroq_auth)\n\u001b[0;32m----> 5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mpipe)\n",
      "File \u001b[0;32m~/.conda/envs/deepseek/lib/python3.10/site-packages/transformers/pipelines/__init__.py:905\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 905\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    916\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.conda/envs/deepseek/lib/python3.10/site-packages/transformers/pipelines/base.py:297\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     framework \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m framework, model\n",
      "File \u001b[0;32m~/.conda/envs/deepseek/lib/python3.10/site-packages/transformers/utils/generic.py:735\u001b[0m, in \u001b[0;36minfer_framework\u001b[0;34m(model_class)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 735\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not infer framework from class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not infer framework from class <class 'langchain_groq.chat_models.ChatGroq'>."
     ]
    }
   ],
   "source": [
    "#from langchain.llms import HuggingFacePipeline\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(temperature=0.8, model_name=\"llama3-8b-8192\", groq_api_key=groq_auth)\n",
    "\n",
    "\n",
    "'''pipe = transformers.pipeline(model=model, tokenizer=tokenizer, torch_dtype=torch.bfloat16, return_full_text=True,  \n",
    "task='text-generation', stopping_criteria=stopping_criteria,  temperature=0.6, max_new_tokens=600, repetition_penalty=1.1)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec97e71-1d3e-4d5d-a8e9-7a1093289945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f417316-c7f7-4452-87cf-c0f6a2a24069",
   "metadata": {},
   "source": [
    "MAIN LOOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8920eaea-548b-461b-ab1b-ebb86afdab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GameRepeat(context, code, floor, loadflag = False):\n",
    "    #initialize game and scenario\n",
    "    scen = open(context +'.txt').read()\n",
    "    game = open(code  +'.txt').read()\n",
    "    \n",
    "    with open('8fixed-results.txt', 'rb') as file:\n",
    "        try:\n",
    "            loc_res = pickle.load(file) \n",
    "        except EOFError:\n",
    "            loc_res = {}\n",
    "            print(\"File empty!\")\n",
    "        \n",
    "    #sys_msg = B_SYS + \"Consider the proposed scenario and act as if you were taking part in it. \\n\"+ scen + E_SYS\n",
    "    #initalize user message\n",
    "    #template = get_prompt(human_msg,  sys_msg)\n",
    "    #prompt = PromptTemplate(input_variables=[\"user_input\"], template=template)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>This is a role-playing game. Consider the proposed scenario and give your answer as if you were taking part in it. \\n \"\"\" + scen + \"\"\"\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Respond to the following using exactly one letter to denote your choice. Your answer must either consist of the letter 'C' for strategy C or 'D' for strategy D.\": {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"]\n",
    "    )\n",
    "\n",
    "    lm = LLMChain(llm=llm,prompt=prompt,verbose=False)\n",
    "    \n",
    "    stratlist = []\n",
    "    if not context+\"_\"+code in loc_res:\n",
    "        loc_res.update({context+\"_\"+code:stratlist})\n",
    "    for i in range(floor,20):\n",
    "        print(i)\n",
    "        rz = lm.predict(question = game)\n",
    "        stratlist.append(rz)\n",
    "        print(rz)\n",
    "        loc_res[context+\"_\"+code].append(rz)\n",
    "        time.sleep(10)\n",
    "        with open('8fixed-results.txt', 'wb') as file:\n",
    "            pickle.dump(loc_res, file)\n",
    "            \n",
    "    return stratlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6537cb9-b7a5-41b1-b0c0-3c6407761d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "C\n",
      "17\n",
      "C\n",
      "18\n",
      "C\n",
      "19\n",
      "C\n",
      "0\n",
      "C\n",
      "1\n",
      "C\n",
      "2\n",
      "C\n",
      "3\n",
      "C\n",
      "4\n",
      "C\n",
      "5\n",
      "C\n",
      "6\n",
      "C\n",
      "7\n",
      "C\n",
      "8\n",
      "C\n",
      "9\n",
      "C\n",
      "10\n",
      "C\n",
      "11\n",
      "C\n",
      "12\n",
      "C\n",
      "13\n",
      "C\n",
      "14\n",
      "C\n",
      "15\n",
      "C\n",
      "16\n",
      "C\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[43mGameRepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m results_8b\u001b[38;5;241m.\u001b[39mupdate({scenario\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mgame:gamma})\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8fixed-results.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn[24], line 37\u001b[0m, in \u001b[0;36mGameRepeat\u001b[0;34m(context, code, floor, loadflag)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(rz)\n\u001b[1;32m     36\u001b[0m loc_res[context\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mcode]\u001b[38;5;241m.\u001b[39mappend(rz)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8fixed-results.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     39\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(loc_res, file)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scenarios = ['IR', 'biz', 'friendsharing', 'environment', 'team'] \n",
    "games = ['prison', 'delight', 'staghunt', 'snowdrift']\n",
    "results_8b = {}\n",
    "\n",
    "try:\n",
    "   with open('8fixed-results.txt', 'rb') as file:\n",
    "       results_8b = pickle.load(file)\n",
    "except Exception as e:\n",
    "    pass\n",
    "       \n",
    "for scenario in scenarios:\n",
    "    for game in games:\n",
    "        lowl = 0\n",
    "        if scenario+\"_\"+game in results_8b:\n",
    "            if len(results_8b[scenario+\"_\"+game]) < 3000:\n",
    "                lowl = len(results_8b[scenario+\"_\"+game])\n",
    "            else:\n",
    "                continue\n",
    "        gamma = GameRepeat(scenario, game, lowl)\n",
    "        results_8b.update({scenario+\"_\"+game:gamma})\n",
    "        with open('8fixed-results.txt', 'wb') as file:\n",
    "            pickle.dump(results_8b, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df3d32b9-f3c6-41ef-b8e7-4c6510fe50eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook project2.ipynb to script\n",
      "[NbConvertApp] Writing 5998 bytes to project2.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert project2.ipynb --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3e27c6-3e85-4e5c-b148-debd79f6acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('7b-fixed-results-oos.txt', 'rb') as file:\n",
    "    loc_res = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7af665-2abe-43fa-b5fb-20aa150baa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sports_prison\n",
      "600\n",
      "sports_delight\n",
      "600\n",
      "sports_staghunt\n",
      "600\n",
      "sports_snowdrift\n",
      "600\n",
      "roomsharing_prison\n",
      "600\n",
      "roomsharing_delight\n",
      "600\n",
      "roomsharing_staghunt\n",
      "600\n",
      "roomsharing_snowdrift\n",
      "600\n",
      "ventcap_prison\n",
      "600\n",
      "ventcap_delight\n",
      "600\n",
      "ventcap_staghunt\n",
      "600\n",
      "ventcap_snowdrift\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "for key in loc_res:\n",
    "    print(key)\n",
    "    print(len(loc_res[key]))\n",
    "\n",
    "#print(loc_res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d22f7-cfea-4340-9edf-eb11ba7cde4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
